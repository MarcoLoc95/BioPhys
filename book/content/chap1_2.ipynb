{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty and errors\n",
    "If there needs to be one thing you must remember from this lesson, let it be this: every measurement or prediction in physics comes with an associated uncertainty. Always, no exception. That is because measurements provide an *estimate* of the true value, not the true value itself. \n",
    "\n",
    "In the context of quantitative measurements, the words **uncertainty** and **error** are sometimes used interchangeably. To be precise, error generally refers to the difference between a measured value and the true or accepted value, while uncertainty is an estimate of the range within which the true value is likely to lie, considering all known errors.\n",
    "\n",
    "We cannot stress it enough: uncertainty is always present, whether we are reporting a measurement or making a prediction from the theory. Therefore it is important to know how to handle it quantitatively, regardless of where your interest in biophysics will take you.\n",
    "\n",
    "The usual way to report a physical quantity $X$ with its best estimate $x$ and its error $e_x$ is:\n",
    "\n",
    "$$\n",
    "X = x \\pm e_x\n",
    "$$\n",
    "\n",
    "\n",
    "> **Philosophical digression**\n",
    ">\n",
    ">But what about the fundamental constants on Nature, like for example the speed of light in vacuum? Do they come with an uncertainty too? In principle no, but only because they become [axioms](https://en.wikipedia.org/wiki/Axiom) of the physical framework used. For this we must be certain that they are *really* constant. In other words, whenever measured properly, the obtained value must always be within the range of uncertainty. In that case and that one only, it can be considered a fixed value of Nature and reported without error. The idea that, if we use better, more sensitive measuring tools, we would still get the same value, just with a smaller uncertainty.\n",
    ">\n",
    ">Not for nothing, the speed of light in vacuum is so constant it is used together with the definition of [second](https://en.wikipedia.org/wiki/Second) to define indirectly what the [meter](https://en.wikipedia.org/wiki/Metre) is. In the next chapter we will talk about units and dimensions, and we will come back at it.\n",
    ">\n",
    "\n",
    "### Types of errors\n",
    "There are countless reasons why we cannot be certain about the values we measure. Consider for instance the measuring of the length of the table: maybe the length changes slightly from side to side due to imperfect manufacturing, or the meter we use cannot reach micrometric precision, or we accidentally measure between two points that are not exactly parallel to the length, or some evil crazy scientist replaced the meter with an inaccurate one, and so on. Some of this errors can be prevented, some other cannot. This does not mean that we cannot be precise in our scientific claims, but only that we acknowledge that what we know is limited to the tools we have to explore Nature. \n",
    "\n",
    "Errors are generally classified between these categories:\n",
    "- **Random errors**: Variations that occur unpredictably from one measurement to another, mostly due to the nature of the process. They cannot be minimized at the source, but they can be reduced by averaging over a large number of observations. An example are the differences in protein concentrations expressed by the same cell line.\n",
    "- **Systematic errors**: These are consistent, repeatable errors due to flaws in measurement instruments or procedures. Most systematic errors take the form of *biases*. They can and should be minimized at the source by properly calibrating the system. Unlike random errors, systematic errors cannot be reduced by increasing the number of observations. An example is not accounting for the mass of the weighing paper when measuring the weight of a powder.\n",
    "- **Blunders**: These are careless human errors that happen randomly and result in (completely) off results. Compared to systematic errors, blunders are due to tiredness or negligence and generate biases hard to quantify. They can be minimized with good attention, careful practices and supervision of other colleagues who can point out the obvious mistakes. An example is pipetting $1\\,mL$ of trypsin instead of $1\\,\\mu L$, or recording a $9$ instead of a $6$ onto a lab report.\n",
    "\n",
    "Some textbooks do not distinguish between systematic errors and blunders; here I want to stress that, while systematic errors can in some cases be corrected in retrospective, blunders should be categorically excluded from analysis.\n",
    " \n",
    "### Instrument resolution\n",
    "Take a ruler and ask yourself: can I measure micrometers with this?\n",
    "\n",
    "The answer is no. That is because the spacing between ticks on the ruler is $1\\,mm$: finer ticks would be hard if not impossible to see with the naked eye, and due to [parallax](https://en.wikipedia.org/wiki/Parallax) they would literally be useless as different observers would report different measurements.\n",
    "\n",
    "The instrument resolution (sometimes called *sensitivity*) represents the smallest increment an instrument can reliably measure, its \"least count\".\n",
    "\n",
    "If we are measuring the length of a table, even after removing all other sources of errors, we cannot get an estimate of the true value more precise than the resolution of the measuring tool used. The only way to reduce this uncertainty is to use more sensitive tools with better resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative error \n",
    "Is $1\\,m$ of uncertainty a lot? If we are measuring the snout-vent length of an animal, for sure; but if we are measuring the distance a bird can migrate in one day, it's basically negligible.\n",
    "\n",
    "For this reason, we can define the **relative error** $r_x$ by putting in relation the error $e_x$ with the measurement $x$ itself:\n",
    "\n",
    "$$\n",
    "r_x = \\frac{e_x}{x}\n",
    "$$\n",
    "\n",
    "The relative error can be expressed either as a real number or as a percentage, both are in principle unambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics of random errors\n",
    "\n",
    "> **Note**\n",
    ">\n",
    ">In the following section you will find some interactive tools to help you visualize the concepts presented.\n",
    ">To run the simulations you first need to click {fa}`rocket` --> {guilabel}`Live Code` on the top right corner of this page, and wait until the kernel has loaded. Then, you need to run the block of code by clicking  {guilabel}`run`.\n",
    ">\n",
    "\n",
    "When we say the outcome of a measurement is \"random\", we do not imply \"unquantifiable\". It is possible that the same object gives different outcomes when measured sequentially in time, or that within a group of objects the measured properties vary in a predetermined range, or both. What is the true value in these cases? What is the uncertainty? \n",
    "\n",
    "Consider for example the height distribution of various tree species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bokeh'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbokeh\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplotting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m figure, show, output_notebook\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbokeh\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnDataSource, CustomJS\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbokeh\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MouseMove\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'bokeh'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, CustomJS, Span, BoxAnnotation, Label\n",
    "from bokeh.events import MouseMove\n",
    "\n",
    "# For inline display in a notebook; if running as a script, consider using output_file(\"interactive.html\")\n",
    "output_notebook()\n",
    "\n",
    "# Load the CSV file and clean column names\n",
    "df = pd.read_csv(\"tree_datasets.csv\")\n",
    "df.columns = [col.strip() for col in df.columns if col.strip() != \"\"]\n",
    "\n",
    "# Define coordinate pairs based on header names: (x, y)\n",
    "pairs = [\n",
    "    (\"Ix\", \"Illicium verum\"),\n",
    "    (\"Mx\", \"Masson pine\"),\n",
    "    (\"Cx\", \"Chinese fir\"),\n",
    "    (\"Sx\", \"Splash pine\")\n",
    "]\n",
    "\n",
    "# Define the Okabeâ€“Ito color palette (for 4 curves)\n",
    "okabe_colors = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\"]\n",
    "\n",
    "# Prepare lists for the renderers and annotation objects, and to store each curve's weighted mean and label\n",
    "renderers = []\n",
    "spans = []\n",
    "left_bands = []\n",
    "right_bands = []\n",
    "labels = []  # for showing mean and std info\n",
    "means = []   # weighted means for each curve\n",
    "\n",
    "# Create a Bokeh figure and set axis labels\n",
    "p = figure(title=\"Tree Height Distribution in Southern China\", width=800, height=400, tools=\"\")\n",
    "p.xaxis.axis_label = \"Height [m]\"\n",
    "p.yaxis.axis_label = \"Relative frequency [a.u.]\"\n",
    "\n",
    "for i, (xcol, ycol) in enumerate(pairs):\n",
    "    if xcol in df.columns and ycol in df.columns:\n",
    "        color = okabe_colors[i]\n",
    "        # Create a data source for the curve\n",
    "        source = ColumnDataSource(data=dict(x=df[xcol], y=df[ycol]))\n",
    "        # Plot the curve with the chosen color\n",
    "        r = p.line('x', 'y', source=source, line_width=2, alpha=1.0, line_color=color, legend_label=f\"{ycol}\")\n",
    "        renderers.append(r)\n",
    "        \n",
    "        # Compute the weighted mean and standard deviation (using y as frequency)\n",
    "        weights = df[ycol]\n",
    "        xs = df[xcol]\n",
    "        mean_val = (xs * weights).sum() / weights.sum()\n",
    "        means.append(mean_val)  # Save for later use in the callback\n",
    "        std_val = np.sqrt((weights * (xs - mean_val)**2).sum() / weights.sum())\n",
    "        \n",
    "        # Interpolate to find the y-value at x = mean_val\n",
    "        mean_y = np.interp(mean_val, xs, df[ycol])\n",
    "        \n",
    "        # Create a vertical dashed line at the weighted mean\n",
    "        span = Span(location=mean_val, dimension='height', line_color=color,\n",
    "                    line_dash='dashed', line_width=2, visible=False)\n",
    "        # Create BoxAnnotations for Â±1 standard deviation around the mean\n",
    "        left_band = BoxAnnotation(left=mean_val - std_val, right=mean_val,\n",
    "                                  fill_color=color, fill_alpha=0.2, visible=False)\n",
    "        right_band = BoxAnnotation(left=mean_val, right=mean_val + std_val,\n",
    "                                   fill_color=color, fill_alpha=0.2, visible=False)\n",
    "        # Create a Label displaying the mean (as <h>) and std (as Ïƒ)\n",
    "        label_text = \"<h>: {:.2f}, Ïƒ: {:.2f}\".format(mean_val, std_val)\n",
    "        label = Label(x=mean_val, y=mean_y, x_offset=30, y_offset=5,\n",
    "                      text=label_text, text_color=color, text_font_size=\"10pt\", visible=False)\n",
    "        \n",
    "        # Add annotations and label to the plot\n",
    "        p.add_layout(span)\n",
    "        p.add_layout(left_band)\n",
    "        p.add_layout(right_band)\n",
    "        p.add_layout(label)\n",
    "        \n",
    "        spans.append(span)\n",
    "        left_bands.append(left_band)\n",
    "        right_bands.append(right_band)\n",
    "        labels.append(label)\n",
    "    else:\n",
    "        print(f\"Column pair ({xcol}, {ycol}) not found in DataFrame.\")\n",
    "\n",
    "# CustomJS callback: highlight the curve whose weighted mean is closest to the mouse's x position.\n",
    "callback_code = \"\"\"\n",
    "let best_index = -1;\n",
    "let best_dist = Infinity;\n",
    "for (let i = 0; i < means.length; i++) {\n",
    "    const dist = Math.abs(cb_obj.x - means[i]);\n",
    "    if (dist < best_dist) {\n",
    "        best_dist = dist;\n",
    "        best_index = i;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Update each curve's opacity and the visibility of its annotations based on the best match.\n",
    "for (let i = 0; i < renderers.length; i++) {\n",
    "    if (i === best_index) {\n",
    "        renderers[i].glyph.line_alpha = 1.0;\n",
    "        spans[i].visible = true;\n",
    "        left_bands[i].visible = true;\n",
    "        right_bands[i].visible = true;\n",
    "        labels[i].visible = true;\n",
    "    } else {\n",
    "        renderers[i].glyph.line_alpha = 0.25;\n",
    "        spans[i].visible = false;\n",
    "        left_bands[i].visible = false;\n",
    "        right_bands[i].visible = false;\n",
    "        labels[i].visible = false;\n",
    "    }\n",
    "}\n",
    "p.change.emit();\n",
    "\"\"\"\n",
    "\n",
    "# Create and attach the callback; pass the renderers, annotations, labels, and means to the JS callback.\n",
    "callback = CustomJS(args=dict(renderers=renderers, spans=spans, left_bands=left_bands, \n",
    "                              right_bands=right_bands, labels=labels, p=p, means=means), code=callback_code)\n",
    "p.js_on_event(MouseMove, callback)\n",
    "\n",
    "p.legend.location = \"top_left\"\n",
    "p.legend.click_policy = \"hide\"\n",
    "\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "*(Data from Wu, Y.; Zhang, X. Object-Based Tree Species Classification Using Airborne Hyperspectral Images and LiDAR Data. Forests 2020, 11, 32.)*\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can describe some general properties of each species, the most obvious being the average height and the spread of the distribution. By hovering your mouse over the curves you will see the **mean** as a vertical dashed line and the **standard deviation** as a colored range around the mean (mean and standard deviation are the accurate statistical terms for average and spread of a distribution, but we'll get there in a minute).\n",
    "\n",
    "You can notice a couple of things from this example:\n",
    "- Nobody would dispute the claim \"A *Masson pine* in southern China grows around 8.6 m tall\". You should therefore not be surprised to see a 8.6 m tall *Masson pine*. The mean is then a good descriptor for the tree distribution;\n",
    "- By looking at the highlighted range by the standard deviation you can see that most trees fall in that range. Similarly to the above claim, you should not be surprised to see a 10.3 m tall Masson pine;\n",
    "- Without knowing absolutely anything about the phenotypes of these trees, if we see a tree 11.0 m tall, there is a very high chance that it is a *Chinese fir* and not a *Illicium verum*. More in general, we can safely claim that *Splash pines* grow taller than *Masson pines*.\n",
    "\n",
    "We can say that the mean is the best estimator for the \"true value\" of the tree height, and the standard deviation the best estimator for the uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation of the \"true value\" from sampled data\n",
    "\n",
    "The **mean** (or average) $<x>$ of a set of sampled data is simply the sum of all the measurements, divided by the number of measurements:\n",
    "\n",
    "$$\n",
    " <x> = \\frac{\\sum_{i}^n x_i}{n}\n",
    "$$\n",
    "\n",
    "The spread of the individual data points from the mean is defined in the **variance** $\\sigma^2$ and can be seen as the average squared distance of individual data points from the mean:\n",
    "\n",
    "$$\n",
    "\\sigma^2 =\\frac{\\sum_{i}^n (x_i-<x>)^2}{n}\n",
    "$$\n",
    "\n",
    "The reason the distance is squared is to avoid that in the sum a positive distance cancels out a negative distance. For example, if we have two measurements $x_1=2$ and $x_2=4$, their mean is $<x>=3$ and the individual distances are $x_1-<x>=-1$ and $x_3-<x>=1$; their sum would be zero and suggest that there is no spread of the data points, which is false.\n",
    "\n",
    "Because the error must be homogeneous with the measurement (have the same units, as we will see later), we define the **sampled standard deviation** $s$ as the square root of the variance:\n",
    "\n",
    "$$\n",
    "s = \\sqrt{\\sigma^2}\n",
    "$$\n",
    "\n",
    "There is one problem with this definition. If we only have one sample, $n=1$ and $<x>=x_1$, so $\\sigma^2=0$ and consequently $s=0$. So this implies that the measurement has no uncertainty? We must introduce a correction to make sure that we have at least two data points to define the variability of the measurement, in what is called the **corrected sampled standard deviation**:\n",
    "\n",
    "$$\n",
    "\\sigma =\\sqrt{\\frac{\\sum_{i}^n (x_i-<x>)^2}{n-1}}\n",
    "$$\n",
    "\n",
    "In this way we also make sure we don't underestimate the spread of the distribution of data points. Notice that, for large amount of samples $n$, the difference between $s$ and $\\sigma$ becomes negligible.\n",
    "\n",
    "Let's now say we collect a huge amount of samples ($n=10000$) to make sure our predictions are as precise as possible. However, the intrinsic statistical variability will not be automatically eliminated; in the example of the trees, we will only get a smoother histogram of the frequencies, it's not like all the trees will change their height to match the mean. But when we then conclude that the \"true value\" must be very close to $<x>$, we know that estimate very well! One or even a hundred trees with very extreme heights would still not significantly change the prediction of what the mean is.\n",
    "\n",
    "We then introduce the **standard error of the mean** $m$ as a measure of \"how well we know the mean\":\n",
    "\n",
    "$$\n",
    "m=\\frac{\\sigma}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "We will not look into the demonstration of this formula, but it has to do with the fact that if we split the dataset in multiple datasets and calculate, for each, the respective mean, the average deviation of the means to the overall mean decreases the more data points we have.\n",
    "\n",
    "For statistical errors, the measurement is reported as follows:\n",
    "\n",
    "$$\n",
    "x=<x>\\pm\\sigma\\qquad \\text{or} \\qquad x=<x>\\pm m\n",
    "$$\n",
    "\n",
    "Most of the time the standard deviation is the way to represent the uncertainty on the measurement, as to say *\"if you pick a random data point from the set, most probably it will lie between $<x>-\\sigma$ and $<x>+\\sigma$*. Using the standard error of the mean instead means *\"the data points can vary from each other, but the mean you expect to calculate will most probably lie between $<x>-m$ and $<x>+m$\"*. In any way the error is chosen to be reported, it must be very clear to the reader which one is used, as misunderstandings can lead to wrong interpretations and conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete random processes\n",
    "Think about how many genes could affect the height of a tree. Biology tells us that height is a complex, polygenic trait: it's not controlled by a single gene, but by the cumulative effect of many genes, each contributing a small amount to the final phenotype. So, even assuming that the chance of a mutation in an individual gene is completely unpredictable and all possible mutations have the same probability (like rolling a die), we end up with a smooth, continuous, bell-shaped distribution of heights. To understand why, we need to look more quantitatively into the math starting from discrete random processes.\n",
    "\n",
    "The most common example of a discrete random (stochastic) process is the roll of a die. While *Dungeons & Dragons* players might be familiar with many different types of dice, everyone else is more acquainted with the cubic die, with 6 equally probable faces (1 to 6, sometimes indicated as 1d6).\n",
    "\n",
    "To describe this process mathematically, we define very generally the *probability $P$ of a measurement $X$ to have an outcome $x$*:\n",
    "\n",
    "$$\n",
    "P\\big(X=x\\big) = \\frac{\\text{number of favourable cases}}{\\text{number of total cases}}\n",
    "$$\n",
    "\n",
    "So, for a regular cubic die (1d6), we have:\n",
    "\n",
    "$$\n",
    "P(1)=P(2)=...=P(6)=\\frac{1}{6}\n",
    "$$\n",
    "\n",
    "What is the probability of getting a 1.5, 0 or a 7? Null! So we can write:\n",
    "\n",
    "$$\n",
    "P(0)=P(1.5)=P(7)=0\n",
    "$$\n",
    "\n",
    "Not very surprising, but technically correct.\n",
    "\n",
    "One important yet trivial property is that the sum of all probabilities must be one:\n",
    "\n",
    "$$\n",
    "\\Sigma P = 1\n",
    "$$\n",
    "\n",
    "In other words, the probability of getting a number between 1 and 6 is 100%. Again, not very surprising.\n",
    "\n",
    "### From discrete to continuous: the central limit theorem\n",
    "Now consider the distribution of the sum obtained by rolling two dice (2d6). The total number of different combinations is $6\\cdot 6=36$.  The simplest way to describe the combined probability is to count all the combinations of possible outcomes:\n",
    "\n",
    "<center>\n",
    "\n",
    "|First die|Second die| Sum| \n",
    "| :-: | :-: | :-: |\n",
    "| 1 | 1 | 2 |\n",
    "| 1 | 2 | 3 | \n",
    "| 1 | 3 | 4 | \n",
    "| ... | ... | ... | \n",
    "| 6 | 4 | 10 |\n",
    "| 6 | 5 | 11 |\n",
    "| 6 | 6 | 12 | \n",
    "\n",
    "</center>\n",
    "\n",
    "There is only one combination that gives sum 2 (1+1), so $P(2)=\\frac{1}{36}$; similarly, there is only one combination that gives sum 12 (6+6), so $P(12)=\\frac{1}{36}$ too. But then, there are many more combinations that give 7 (1+6, 2+5, 3+4, 4+3, 5+2, 6+1) so $P(7)=\\frac{6}{36}$. Something interesting happens: combining perfect equal distributions of probability, we get a distribution that is not equal at all! This new distribution, for the mathematically inclined, is called **binomial distribution**.\n",
    "\n",
    "Try to play now with the number of dice you sum and see how the probability distribution changes: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "def slider_with_units(value, min_val, max_val, step, readout_format, description, unit):\n",
    "    slider = widgets.FloatSlider(\n",
    "        value=value, \n",
    "        min=min_val, \n",
    "        max=max_val, \n",
    "        step=step,\n",
    "        readout=False,\n",
    "        description=f\"{description} = {value:{readout_format}} {unit}\",\n",
    "        style={'description_width': '100px'},\n",
    "        layout=widgets.Layout(width='500px')\n",
    "    )\n",
    "    slider.observe(lambda change: slider.set_trait('description', \n",
    "                           f\"{description} = {change['new']:{readout_format}} {unit} \"), names='value')\n",
    "    return slider\n",
    "\n",
    "def plot_sum(n):\n",
    "    n = int(n)\n",
    "    # Compute the distribution of the sum of n dice by convolving the PMF of one die n times.\n",
    "    # Each die has outcomes 1 through 6 with equal probability.\n",
    "    p_die = np.ones(6) / 6\n",
    "    p = p_die.copy()\n",
    "    for _ in range(n - 1):\n",
    "        p = np.convolve(p, p_die)\n",
    "        \n",
    "    # The possible sums range from n (all dice show 1) to 6*n (all dice show 6)\n",
    "    sums = np.arange(n, 6*n + 1)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    # Plot black dots for each probability\n",
    "    plt.plot(sums, p, 'ko')\n",
    "    # Draw a thin vertical line from each dot to the x-axis\n",
    "    for s, prob in zip(sums, p):\n",
    "        plt.vlines(s, 0, prob, colors='k', linestyles='solid', linewidth=0.5)\n",
    "    \n",
    "    plt.xlabel(\"Sum\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xlim(n, 6*n)\n",
    "    plt.ylim(0, max(p) * 1.1)\n",
    "    # Remove grid, title, and legend as requested\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_sum,\n",
    "         n=slider_with_units(1, 1, 50, 1, '.0f', \"n dice\", \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even from using only four dice we get a distribution that recalls a bell. The higher the number of dice, or stochastic factors in general, the smoother and more defined the distribution of outcomes will look like. And, in the limit of very large number of dice, the distribution will follow what is called a **Gaussian distribution**. This tendency of binomials to approximate a Gaussian curve is called **central limit theorem** and can be proven mathematically.\n",
    "\n",
    "Wait, is this Gaussian distribution like the distribution of tree heights? Yes indeed. We mentioned that many (hundreds? thousands?) individual genes determine the height, and it makes perfect sense in a world where the phenotype is largely determined by the genotype. The Gaussian distribution is so common in Nature that is called the **\"normal distribution\"**. When a stochastic process is influenced by countless random factors, most probably its distribution will follow a Gaussian curve.\n",
    "\n",
    "If your attention levels are still high, you might have noticed that the maximum probability decreases as the number of randomic factor increases. That is because, when rolling 50 dice, the probability of getting *exactly* 175 is very little: $P(35)=0.033$. That is because 174 and 176 are also quite probable. So, if we want to have a meaningful description of continuous stochastic processes, we need to slightly adjust the math accordingly.\n",
    "\n",
    "### Continuous random processes\n",
    "It is completely pointless to ask what is the probability of a tree being *exactly* 8 m tall. That's because if the height is infinitesimally different from 8, like 8.000000...0001 m, it would not count as a favorauble outcome. So if we calculate it in the continuous case, we would get $P(8)\\approx 0$. What is more useful is to ask, instead, what is the probability of an outcome being between a certain range $x$ and $x+\\Delta x$. \n",
    "If the $\\Delta x$ is very small, we can write:\n",
    "\n",
    "$$\n",
    "\\Delta x \\to dx, \\qquad P(x\\le X \\le x+dx) \\to p(x)dx\n",
    "$$\n",
    "\n",
    "where $p(x)$ (notice the lowercase) is a continuous function describing the shape of the distribution, called **probability density**.\n",
    "\n",
    "Then to get the total probability in a certain range one can sum all the infinitesimal probabilities:\n",
    "\n",
    "$$\n",
    "P(x\\le X \\le x+\\Delta x)=\\int_x^{x+\\Delta x} p(x)dx\n",
    "$$\n",
    "\n",
    "Similarly to the discrete distributions, the normalization condition is:\n",
    "$$\n",
    "\\int_{-\\infty}^{+\\infty} p(x)dx=1\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for sampled data the mean, variance and standard deviation were discrete sums, for continuous stochastic processes they must become integrals, respectively:\n",
    "\n",
    "$$\n",
    "<x>=\\int_{-\\infty}^\\infty x p(x) dx, \\qquad \\sigma^2=\\int_{-\\infty}^\\infty (x-<x>)^2 p(x)dx = <x^2>-<x>^2, \\qquad \\sigma=\\sqrt{\\sigma^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now that we have a framework for continuous distributions, we can say that the central limit theorem describes the probability density for a Gaussian stochastic process:\n",
    "\n",
    "$$\n",
    "p(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-<x>)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "where $<x>$ is the mean and $\\sigma$ the standard deviation (both these can be proven by calculating the above integral definitions).\n",
    "\n",
    "What is the probability that the outcome of a measurement is between $<x>-\\sigma$ and $<x>+\\sigma$? We can calculate it easily now:\n",
    "\n",
    "$$\n",
    " P(<x>-\\sigma\\le X \\le <x>+\\sigma) =\\int_{<x>-\\sigma}^{<x>+\\sigma} p(x) \\approx 0.68\n",
    "$$\n",
    "\n",
    "That's more than $2/3$ of the data points! One can calculate the probabilities for $2\\sigma$ and $3\\sigma$:\n",
    "\n",
    "$$\n",
    " P(<x>-2\\sigma\\le X \\le <x>+2\\sigma) \\approx 0.95\n",
    "$$\n",
    "\n",
    "$$\n",
    " P(<x>-3\\sigma\\le X \\le <x>+3\\sigma) \\approx 0.997\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Back to errors: which one to choose?\n",
    "Say the sensitivity of an instrument is $S$. By using this instrument we collect many data points and we calculate the standard deviation $\\sigma$. Which value to use as an uncertainty?\n",
    "\n",
    "The short answer is: the largest value between $S$ and $\\sigma$. That's because if the standard deviation is below the sensitivity, the individual variations may just be due to the imprecision of the instrument and not to a natural random variability. However, if the standard deviation is greater than the sensitivity, we know that the individual variations are all well measured with the instrument, despite its limited resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error propagation\n",
    "\n",
    "Imagine you measured the motion of kinesin along microtubules. You have quantified the traveled distance $x$ and the elapsed time $t$, each with their respective uncertainty:\n",
    "\n",
    "$$\n",
    "x=<x>\\pm e_x,  \\qquad t=<t>\\pm e_t\n",
    "$$\n",
    "\n",
    "You calculate the average velocity as $v=x/t$. But if you know the distance and the time with a margin of error, how well do you know the velocity?\n",
    "\n",
    "Intuitively, one could calculate the minimum and maximum velocity:\n",
    "\n",
    "$$\n",
    " v_{min}=\\frac{<x>- e_x}{<t>+e_t}, \\qquad v_{max}=\\frac{<x>+ e_x}{<t>-e_t}\n",
    "$$\n",
    "\n",
    "Any possible combination of $x$ and $t$ within their ranges of uncertainty will fall between $v_{min}$ and $v_{max}$. But to report the measurement we need an estimate $<v>$ and an error bar $e_v$ around it. We could in principle use $v_{min}$ and $v_{max}$:\n",
    "\n",
    "$$\n",
    "<v>=\\frac{v_{min}+v_{max}}{2}=\\frac{<x><t>+e_xe_t}{<t>^2-e_t^2}, \\qquad e_v=\\frac{v_{max}-v_{min}}{2}= \\frac{e_x<t>+e_t<x>}{<t>^2-e_t^2}\n",
    "$$\n",
    "\n",
    "This approach is not wrong *per se*, but it can become quite complicated for formulas where minimum and maximum are not so easy to calculate. Another minor issue is that, by writing $v=<v>\\pm e_v$ we are implying that any value between $v_{min}$ and $v_{max}$ is equally probable, which is not exactly true: most combinations of $x$ and $t$ will give a value of $v$ closer to $v_{min}$ then to $v_{max}$.\n",
    "\n",
    "A more reliable and general method involves the *linearization* of the formula through a first-order multivariate [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series), which simplifies the treatment regardless of the complexity of the original formula.\n",
    "\n",
    "\n",
    "### Refresher on Taylor expansions\n",
    "You may recall that, for any function having only one variable, the Taylor series around the point $x_0$ is:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_n \\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n\n",
    "$$\n",
    "\n",
    "where $f^{(n)}(x_0)$ is the $n$-th derivative of $f(x)$ calculated in $x_0$ and $n!$ is the [factorial](https://en.wikipedia.org/wiki/Factorial) of n.\n",
    "\n",
    "The first-order expansion of $f(x)$ around $x_0$ can be calculated by taking $n=1$:\n",
    "\n",
    "$$\n",
    "f(x)\\approx f(x_0)+\\left.\\frac{df}{dx}\\right|_{x=x_0} (x-x_0)\n",
    "$$\n",
    "\n",
    "where the notation $\\left.\\frac{df}{dx}\\right|_{x=x_0}$ means \"the first derivative of $f$ in $x$ calculated in $x=x_0$\".\n",
    "\n",
    "If $f$ is a function of **two** variables $x_1$ and $x_2$, then the first-order expansion around $(x_{1,0},x_{2,0})$ is, analogously:\n",
    "\n",
    "$$\n",
    "f(x_1,x_2)\\approx f(x_{1,0},x_{2,0})+\\left.\\frac{\\partial f}{\\partial x_1}\\right|_{\\substack{x_1=x_{1,0} \\\\ x_2=x_{2,0}}} (x_1-x_{1,0})+\\left.\\frac{\\partial f}{\\partial x_2}\\right|_{\\substack{x_1=x_{1,0} \\\\ x_2=x_{2,0}}} (x_2-x_{2,0})\n",
    "$$\n",
    "\n",
    "where the symbol $\\frac{\\partial}{\\partial x_i}$ indicates the *partial derivative* of a function, that is the derivative only in that variable while keeping all the others constant.\n",
    "\n",
    "Most generally, if $f$ is a function of $m$ variables $x_1,...,x_m$ around $(x_{1,0},..,x_{m,0})$:\n",
    "\n",
    "$$\n",
    "f(x_1,..,x_m)\\approx f(x_{1,0},..,x_{m,0})+\\sum_i^m \\left.\\frac{\\partial f}{\\partial x_i}\\right|_{\\substack{x_1=x_{1,0} \\\\ ... \\\\ x_m=x_{m,0}}} (x_i-x_{i,0})\n",
    "$$\n",
    "\n",
    "### Proof of the formula for two variables\n",
    "For simplicity, let's start with the formula with two variables $x_1$ and $x_2$ with errors $e_1$ and $e_2$ (just like the first example about the velocity). We can notice that the difference $x_i-x_{i,0}$ for every $i$ is the deviation from the \"true value\" $x_{i,0}$, or in other words the definition of the error $e_i$.\n",
    "\n",
    "$$\n",
    "f(x_1,x_2)\\approx f(x_{1,0},x_{2,0})+\\left.\\frac{\\partial f}{\\partial x_1}\\right|_{\\substack{x_1=x_{1,0} \\\\ x_2=x_{2,0}}} \\underbrace{(x_1-x_{1,0})}_{\\pm e_1}+\\left.\\frac{\\partial f}{\\partial x_2}\\right|_{\\substack{x_1=x_{1,0} \\\\ x_2=x_{2,0}}} \\underbrace{(x_2-x_{2,0})}_{\\pm e_2}\n",
    "$$\n",
    "\n",
    "But then we see that the quantity $f$ is reported in the same way as we report a measurement with its error:\n",
    "\n",
    "$$\n",
    "f=f(x_{1,0},x_{2,0})\\pm e_f\n",
    "$$\n",
    "\n",
    "We then have a relation between the formulas that defines $e_f$ for us! All those pluses and minuses, however, are a bit hard to handle. Let's then square $e_v$ to remove some signs:\n",
    "\n",
    "$$\n",
    "e_f^2=\\bigg(\\left.\\frac{\\partial f}{\\partial x_1}\\right|_{\\substack{x_1=x_{1,0} \\\\ x_2=x_{2,0}}}\\bigg)^2 e_1^2 \\pm 2 \\bigg(\\left.\\frac{\\partial f}{\\partial x_1}\\right|_{\\substack{x_1=x_{1,0} \\\\ x_2=x_{2,0}}}\\bigg) \\bigg(\\left.\\frac{\\partial f}{\\partial x_2}\\right|_{\\substack{x_1=x_{1,0} \\\\ x_2=x_{2,0}}}\\bigg) e_1e_2 + \\bigg(\\left.\\frac{\\partial f}{\\partial x_2}\\right|_{\\substack{x_1=x_{1,0} \\\\ x_2=x_{2,0}}}\\bigg)^2e_2^2\n",
    "$$\n",
    "\n",
    "We won't prove it mathematically, but it can be shown that the central term tends to zero when we take multiple measurements, because the errors $e_1$ and $e_2$ are independent from each other.\n",
    "\n",
    "We therefore get:\n",
    "\n",
    "$$\n",
    "e_f^2=\\bigg(\\left.\\frac{\\partial f}{\\partial x_1}\\right|_{\\substack{x_1=x_{1,0} \\\\ x_2=x_{2,0}}}\\bigg)^2 e_1^2 + \\bigg(\\left.\\frac{\\partial f}{\\partial x_2}\\right|_{\\substack{x_1=x_{1,0} \\\\ x_2=x_{2,0}}}\\bigg)^2e_2^2\n",
    "$$\n",
    "\n",
    "and we finally get a formula for $e_f$:\n",
    "\n",
    "$$\n",
    "e_f=\\sqrt{\\bigg(\\left.\\frac{\\partial f}{\\partial x_1}\\right|_{\\substack{x_1=x_{1,0} \\\\ x_2=x_{2,0}}}\\bigg)^2 e_1^2 + \\bigg(\\left.\\frac{\\partial f}{\\partial x_2}\\right|_{\\substack{x_1=x_{1,0} \\\\ x_2=x_{2,0}}}\\bigg)^2e_2^2}\n",
    "$$\n",
    "\n",
    "### General formula for calculated errors\n",
    "The generalized version of the formula, the one worth remembering and writing in your formula sheet, is:\n",
    "\n",
    "$$\n",
    "e_f=\\sqrt{\\sum_i \\bigg(\\frac{\\partial f}{\\partial x_i}\\bigg)^2 e_i^2}\n",
    "$$\n",
    "\n",
    "While it might look complicated, the procedure to calculate it is the following:\n",
    "1. From the formula of $f$, identify the measured quantitites as they will be the $x_i$;\n",
    "2. Calculate separately each partial derivative $\\frac{\\partial f}{\\partial x_i}$;\n",
    "3. Put in the numbers and calculate $e_f$.\n",
    "\n",
    ">\n",
    "> **Example**\n",
    ">\n",
    "> Coming back to the example of the average velocity, let's say you measured $x=(2.0\\pm 0.5)\\,\\mu m$ and $t=(5\\pm 0.1)\\,s$.\n",
    ">\n",
    "> 1. The formula for the average velocity is $v=\\frac{x}{t}$. The two variables we measured are $x$ and $t$.\n",
    ">\n",
    "> 2. We calculate the partial derivatives $\\frac{\\partial v}{\\partial x}=\\frac{1}{t}$ and $\\frac{\\partial v}{\\partial t}=-\\frac{x}{t^2}$\n",
    ">\n",
    "> 3. We put in the numbers $e_v=\\sqrt{\\big(\\frac{1}{5}\\big)^2 0.5^2 + \\big(\\frac{2}{5^2}\\big)^2 0.1^2}\\,\\mu m/s=\\sqrt{0.01+0.000064}\\,\\mu m/s\\approx 0.1\\,\\mu m/s$\n",
    ">\n",
    "> We report the velocity as $v=(0.4\\pm 0.1)\\,\\mu m/s$\n",
    ">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
