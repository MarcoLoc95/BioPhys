{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty and errors\n",
    "If there needs to be one thing you must remember from this lesson, let it be this: every measurement or prediction in physics comes with an associated uncertainty. Always, no exception. That is because measurements provide an *estimate* of the true value, not the true value itself. \n",
    "\n",
    "In the context of quantitative measurements, the words **uncertainty** and **error** are sometimes used interchangeably. To be precise, error generally refers to the difference between a measured value and the true or accepted value, while uncertainty is an estimate of the range within which the true value is likely to lie, considering all known errors.\n",
    "\n",
    "We cannot stress it enough: uncertainty is always present, whether we are reporting a measurement or making a prediction from the theory. Therefore it is important to know how to handle it quantitatively, regardless of where your interest in biophysics will take you.\n",
    "\n",
    "> **Philosophical digression**\n",
    ">\n",
    ">But what about the fundamental constants on Nature, like for example the speed of light in vacuum? Do they come with an uncertainty too? In principle no, but only because they become [axioms](https://en.wikipedia.org/wiki/Axiom) of the physical framework used. For this we must be certain that they are *really* constant. In other words, whenever measured properly, the obtained value must always be within the range of uncertainty. In that case and that one only, it can be considered a fixed value of Nature and reported without error. The idea that, if we use better, more sensitive measuring tools, we would still get the same value, just with a smaller uncertainty.\n",
    ">\n",
    ">Not for nothing, the speed of light in vacuum is so constant it is used together with the definition of [second](https://en.wikipedia.org/wiki/Second) to define indirectly what the [meter](https://en.wikipedia.org/wiki/Metre) is. In the next chapter we will talk about units and dimensions, and we will come back at it.\n",
    ">\n",
    "\n",
    "### Types of errors\n",
    "There are countless reasons why we cannot be certain about the values we measure. Consider for instance the measuring of the length of the table: maybe the length changes slightly from side to side due to imperfect manufacturing, or the meter we use cannot reach micrometric precision, or we accidentally measure between two points that are not exactly parallel to the length, or some evil crazy scientist replaced the meter with an inaccurate one, and so on. Some of this errors can be prevented, some other cannot. This does not mean that we cannot be precise in our scientific claims, but only that we acknowledge that what we know is limited to the tools we have to explore Nature. \n",
    "\n",
    "Errors are generally classified between these categories:\n",
    "- **Random errors**: Variations that occur unpredictably from one measurement to another, mostly due to the nature of the process. They cannot be minimized at the source, but they can be reduced by averaging over a large number of observations. An example are the differences in protein concentrations expressed by the same cell line.\n",
    "- **Systematic errors**: These are consistent, repeatable errors due to flaws in measurement instruments or procedures. Most systematic errors take the form of *biases*. They can and should be minimized at the source by properly calibrating the system. Unlike random errors, systematic errors cannot be reduced by increasing the number of observations. An example is not accounting for the mass of the weighing paper when measuring the weight of a powder.\n",
    "- **Blunders**: These are careless human errors that happen randomly and result in (completely) off results. Compared to systematic errors, blunders are due to tiredness or negligence and generate biases hard to quantify. They can be minimized with good attention, careful practices and supervision of other colleagues who can point out the obvious mistakes. An example is pipetting $1\\,mL$ of trypsin instead of $1\\,\\mu L$, or recording a $9$ instead of a $6$ onto a lab report.\n",
    "\n",
    "Some textbooks do not distinguish between systematic errors and blunders; here I want to stress that, while systematic errors can in some cases be corrected in retrospective, blunders should be categorically excluded from analysis.\n",
    " \n",
    "### Instrument resolution\n",
    "Take a ruler and ask yourself: can I measure micrometers with this?\n",
    "\n",
    "The answer is no. That is because the spacing between ticks on the ruler is $1\\,mm$: finer ticks would be hard if not impossible to see with the naked eye, and due to [parallax](https://en.wikipedia.org/wiki/Parallax) they would literally be useless as different observers would report different measurements.\n",
    "\n",
    "The instrument resolution (sometimes called *sensitivity*) represents the smallest increment an instrument can reliably measure, its \"least count\".\n",
    "\n",
    "If we are measuring the length of a table, even after removing all other sources of errors, we cannot get an estimate of the true value more precise than the resolution of the measuring tool used. The only way to reduce this uncertainty is to use more sensitive tools with better resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative error \n",
    "explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics of random errors\n",
    "\n",
    "> **Note**\n",
    ">\n",
    ">In the following section you will find some interactive tools to help you visualize the concepts presented.\n",
    ">To run the simulations you first need to click {fa}`rocket` --> {guilabel}`Live Code` on the top right corner of this page, and wait until the kernel has loaded. Then, you need to run the block of code by clicking  {guilabel}`run`.\n",
    ">\n",
    "\n",
    "When we say the outcome of a measurement is \"random\", we do not imply \"unquantifiable\". It is possible that the same object gives different outcomes when measured sequentially in time, or that within a group of objects the measured properties vary in a predetermined range, or both. What is the true value in these cases? What is the uncertainty? \n",
    "\n",
    "Consider for example the height distribution of various tree species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bokeh'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbokeh\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplotting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m figure, show, output_notebook\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbokeh\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnDataSource, CustomJS\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbokeh\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MouseMove\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'bokeh'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, CustomJS, Span, BoxAnnotation, Label\n",
    "from bokeh.events import MouseMove\n",
    "\n",
    "# For inline display in a notebook; if running as a script, consider using output_file(\"interactive.html\")\n",
    "output_notebook()\n",
    "\n",
    "# Load the CSV file and clean column names\n",
    "df = pd.read_csv(\"tree_datasets.csv\")\n",
    "df.columns = [col.strip() for col in df.columns if col.strip() != \"\"]\n",
    "\n",
    "# Define coordinate pairs based on header names: (x, y)\n",
    "pairs = [\n",
    "    (\"Ix\", \"Illicium verum\"),\n",
    "    (\"Mx\", \"Masson pine\"),\n",
    "    (\"Cx\", \"Chinese fir\"),\n",
    "    (\"Sx\", \"Splash pine\")\n",
    "]\n",
    "\n",
    "# Define the Okabe–Ito color palette (for 4 curves)\n",
    "okabe_colors = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\"]\n",
    "\n",
    "# Prepare lists for the renderers and annotation objects, and to store each curve's weighted mean and label\n",
    "renderers = []\n",
    "spans = []\n",
    "left_bands = []\n",
    "right_bands = []\n",
    "labels = []  # for showing mean and std info\n",
    "means = []   # weighted means for each curve\n",
    "\n",
    "# Create a Bokeh figure and set axis labels\n",
    "p = figure(title=\"Tree Height Distribution in Southern China\", width=800, height=400, tools=\"\")\n",
    "p.xaxis.axis_label = \"Height [m]\"\n",
    "p.yaxis.axis_label = \"Relative frequency [a.u.]\"\n",
    "\n",
    "for i, (xcol, ycol) in enumerate(pairs):\n",
    "    if xcol in df.columns and ycol in df.columns:\n",
    "        color = okabe_colors[i]\n",
    "        # Create a data source for the curve\n",
    "        source = ColumnDataSource(data=dict(x=df[xcol], y=df[ycol]))\n",
    "        # Plot the curve with the chosen color\n",
    "        r = p.line('x', 'y', source=source, line_width=2, alpha=1.0, line_color=color, legend_label=f\"{ycol}\")\n",
    "        renderers.append(r)\n",
    "        \n",
    "        # Compute the weighted mean and standard deviation (using y as frequency)\n",
    "        weights = df[ycol]\n",
    "        xs = df[xcol]\n",
    "        mean_val = (xs * weights).sum() / weights.sum()\n",
    "        means.append(mean_val)  # Save for later use in the callback\n",
    "        std_val = np.sqrt((weights * (xs - mean_val)**2).sum() / weights.sum())\n",
    "        \n",
    "        # Interpolate to find the y-value at x = mean_val\n",
    "        mean_y = np.interp(mean_val, xs, df[ycol])\n",
    "        \n",
    "        # Create a vertical dashed line at the weighted mean\n",
    "        span = Span(location=mean_val, dimension='height', line_color=color,\n",
    "                    line_dash='dashed', line_width=2, visible=False)\n",
    "        # Create BoxAnnotations for ±1 standard deviation around the mean\n",
    "        left_band = BoxAnnotation(left=mean_val - std_val, right=mean_val,\n",
    "                                  fill_color=color, fill_alpha=0.2, visible=False)\n",
    "        right_band = BoxAnnotation(left=mean_val, right=mean_val + std_val,\n",
    "                                   fill_color=color, fill_alpha=0.2, visible=False)\n",
    "        # Create a Label displaying the mean (as <h>) and std (as σ)\n",
    "        label_text = \"<h>: {:.2f}, σ: {:.2f}\".format(mean_val, std_val)\n",
    "        label = Label(x=mean_val, y=mean_y, x_offset=30, y_offset=5,\n",
    "                      text=label_text, text_color=color, text_font_size=\"10pt\", visible=False)\n",
    "        \n",
    "        # Add annotations and label to the plot\n",
    "        p.add_layout(span)\n",
    "        p.add_layout(left_band)\n",
    "        p.add_layout(right_band)\n",
    "        p.add_layout(label)\n",
    "        \n",
    "        spans.append(span)\n",
    "        left_bands.append(left_band)\n",
    "        right_bands.append(right_band)\n",
    "        labels.append(label)\n",
    "    else:\n",
    "        print(f\"Column pair ({xcol}, {ycol}) not found in DataFrame.\")\n",
    "\n",
    "# CustomJS callback: highlight the curve whose weighted mean is closest to the mouse's x position.\n",
    "callback_code = \"\"\"\n",
    "let best_index = -1;\n",
    "let best_dist = Infinity;\n",
    "for (let i = 0; i < means.length; i++) {\n",
    "    const dist = Math.abs(cb_obj.x - means[i]);\n",
    "    if (dist < best_dist) {\n",
    "        best_dist = dist;\n",
    "        best_index = i;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Update each curve's opacity and the visibility of its annotations based on the best match.\n",
    "for (let i = 0; i < renderers.length; i++) {\n",
    "    if (i === best_index) {\n",
    "        renderers[i].glyph.line_alpha = 1.0;\n",
    "        spans[i].visible = true;\n",
    "        left_bands[i].visible = true;\n",
    "        right_bands[i].visible = true;\n",
    "        labels[i].visible = true;\n",
    "    } else {\n",
    "        renderers[i].glyph.line_alpha = 0.25;\n",
    "        spans[i].visible = false;\n",
    "        left_bands[i].visible = false;\n",
    "        right_bands[i].visible = false;\n",
    "        labels[i].visible = false;\n",
    "    }\n",
    "}\n",
    "p.change.emit();\n",
    "\"\"\"\n",
    "\n",
    "# Create and attach the callback; pass the renderers, annotations, labels, and means to the JS callback.\n",
    "callback = CustomJS(args=dict(renderers=renderers, spans=spans, left_bands=left_bands, \n",
    "                              right_bands=right_bands, labels=labels, p=p, means=means), code=callback_code)\n",
    "p.js_on_event(MouseMove, callback)\n",
    "\n",
    "p.legend.location = \"top_left\"\n",
    "p.legend.click_policy = \"hide\"\n",
    "\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "*(Data from Wu, Y.; Zhang, X. Object-Based Tree Species Classification Using Airborne Hyperspectral Images and LiDAR Data. Forests 2020, 11, 32.)*\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still describe some general properties of each species, the most obvious being the average height and the spread of the distribution. By hovering your mouse over the curves you will see the **mean** as a vertical dashed line and the **standard deviation** as a colored range around the mean (mean and standard deviation are the accurate statistical terms for average and spread of a distribution, but we'll get there in a minute).\n",
    "\n",
    "You can notice a couple of things from this example:\n",
    "- Nobody would dispute the claim \"A *Masson pine* in southern China grows around 8.6 m tall\". You should therefore not be surprised to see a 8.6 m tall *Masson pine*. The mean is then a good descriptor for the tree distribution;\n",
    "- By looking at the highlighted range by the standard deviation you can see that most trees fall in that range. Similarly to the above claim, you should not be surprised to see a 10.3 m tall Masson pine;\n",
    "- Without knowing absolutely anything about the phenotypes of these trees, if we see a tree 11.0 m tall, there is a very high chance that it is a *Chinese fir* and not a *Illicium verum*. More in general, we can safely claim that *Splash pines* grow taller than *Masson pines*.\n",
    "\n",
    "We can say that the mean is the best estimator for the \"true value\" of the tree height, and the standard deviation the best estimator for the uncertainty. There you go, you have the recipe for describing statistical errors! But why? Are there better ways to describe statistical errors?\n",
    "\n",
    "Think about how many genes could affect the height of a tree. Biology tells us that height is a complex, polygenic trait: it's not controlled by a single gene, but by the cumulative effect of many genes, each contributing a small amount to the final phenotype. So, even assuming that the chance of a mutation in an individual gene is completely unpredictable and all possible mutations have the same probability (like rolling a die), we end up with a smooth, continuous, bell-shaped distribution of heights. To understand why we need to look more quantitatively into the math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation of the \"true value\" from sampled data\n",
    "Mean, variance, standard deviation, corrected standard deviation, Standard error of the mean $m$\n",
    "\n",
    "x=<x>+-std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete random processes\n",
    "The most common example of a discrete random (stochastic) process is the roll of a die. While *Dungeons & Dragons* players might be familiar with many different types of dice, the most common is the cubic die, with 6 equally probable faces (1 to 6).\n",
    "\n",
    "To describe this process mathematically, we define very generally the *probability $P$ of a measurement $X$ to have an outcome $x$*:\n",
    "\n",
    "$$\n",
    "P\\big(X=x\\big) = \\frac{\\text{number of favourable cases}}{\\text{number of total cases}}\n",
    "$$\n",
    "\n",
    "So, for a regular cubic die (1d6), we have:\n",
    "\n",
    "$$\n",
    "P(1)=P(2)=...=P(6)=\\frac{1}{6}\n",
    "$$\n",
    "\n",
    "What is the probability of getting a 1.5, 0 or a 7? Null! So we can write:\n",
    "\n",
    "$$\n",
    "P(0)=P(1.5)=P(7)=0\n",
    "$$\n",
    "\n",
    "Not very surprising, but technically correct.\n",
    "\n",
    "One important yet trivial property is that the sum of all probabilities must be one:\n",
    "\n",
    "$$\n",
    "\\Sigma P = 1\n",
    "$$\n",
    "\n",
    "In other words, the probability of getting a number between 1 and 6 is 100%. Again, not very surprising.\n",
    "\n",
    "### From discrete to continuous: the central limit theorem\n",
    "Now consider the distribution of the sum obtained by rolling two dice (2d6). The total number of different combinations is $6\\cdot 6=36$.  The simplest way to describe the combined probability is to count all the combinations of possible outcomes:\n",
    "\n",
    "<center>\n",
    "\n",
    "|First die|Second die| Sum| \n",
    "| :-: | :-: | :-: |\n",
    "| 1 | 1 | 2 |\n",
    "| 1 | 2 | 3 | \n",
    "| 1 | 3 | 4 | \n",
    "| ... | ... | ... | \n",
    "| 6 | 4 | 10 |\n",
    "| 6 | 5 | 11 |\n",
    "| 6 | 6 | 12 | \n",
    "\n",
    "</center>\n",
    "\n",
    "There is only one combination that gives sum 2 (1+1), so $P(2)=\\frac{1}{36}$; similarly, there is only one combination that gives sum 12 (6+6), so $P(12)=\\frac{1}{36}$ too. But then, there are many more combinations that give 7 (1+6, 2+5, 3+4, 4+3, 5+2, 6+1) so $P(7)=\\frac{6}{36}$. Something interesting happens: combining perfect equal distributions of probability, we get a distribution that is not equal at all! This new distribution, for the mathematically inclined, is called **binomial distribution**.\n",
    "\n",
    "Try to play now with the number of dice you sum and see how the probability distribution changes: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "def slider_with_units(value, min_val, max_val, step, readout_format, description, unit):\n",
    "    slider = widgets.FloatSlider(\n",
    "        value=value, \n",
    "        min=min_val, \n",
    "        max=max_val, \n",
    "        step=step,\n",
    "        readout=False,\n",
    "        description=f\"{description} = {value:{readout_format}} {unit}\",\n",
    "        style={'description_width': '100px'},\n",
    "        layout=widgets.Layout(width='500px')\n",
    "    )\n",
    "    slider.observe(lambda change: slider.set_trait('description', \n",
    "                           f\"{description} = {change['new']:{readout_format}} {unit} \"), names='value')\n",
    "    return slider\n",
    "\n",
    "def plot_sum(n):\n",
    "    n = int(n)\n",
    "    # Compute the distribution of the sum of n dice by convolving the PMF of one die n times.\n",
    "    # Each die has outcomes 1 through 6 with equal probability.\n",
    "    p_die = np.ones(6) / 6\n",
    "    p = p_die.copy()\n",
    "    for _ in range(n - 1):\n",
    "        p = np.convolve(p, p_die)\n",
    "        \n",
    "    # The possible sums range from n (all dice show 1) to 6*n (all dice show 6)\n",
    "    sums = np.arange(n, 6*n + 1)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    # Plot black dots for each probability\n",
    "    plt.plot(sums, p, 'ko')\n",
    "    # Draw a thin vertical line from each dot to the x-axis\n",
    "    for s, prob in zip(sums, p):\n",
    "        plt.vlines(s, 0, prob, colors='k', linestyles='solid', linewidth=0.5)\n",
    "    \n",
    "    plt.xlabel(\"Sum\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xlim(n, 6*n)\n",
    "    plt.ylim(0, max(p) * 1.1)\n",
    "    # Remove grid, title, and legend as requested\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_sum,\n",
    "         n=slider_with_units(1, 1, 50, 1, '.0f', \"n dice\", \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even from using only four dice we get a distribution that recalls a bell. The higher the number of dice, or stochastic factors in general, the smoother and more defined the distribution of outcomes will look like. And, in the limit of very large number of dice, the distribution will follow what is called a **Gaussian distribution**. This tendency of binomials to approximate a Gaussian curve is called **central limit theorem** and can be proven mathematically.\n",
    "\n",
    "Wait, is this Gaussian distribution like the distribution of tree heights? Yes indeed. We mentioned that many (hundreds? thousands?) individual genes determine the height, and it makes perfect sense in a world where the phenotype is largely determined by the genotype. The Gaussian distribution is so common in Nature that is called the **\"normal distribution\"**. When a stochastic process is influenced by countless random factors, most probably its distribution will follow a Gaussian curve.\n",
    "\n",
    "If your attention levels are still high, you might have noticed that the maximum probability decreases as the number of randomic factor increases. That is because, when rolling 50 dice, the probability of getting *exactly* 175 is very little: $P(35)=0.033$. That is because 174 and 176 are also quite probable. So, if we want to have a meaningful description of continuous stochastic processes, we need to slightly adjust the math accordingly.\n",
    "\n",
    "### Continuous random processes\n",
    "It is completely pointless to ask what is the probability of a tree being *exactly* 8 m tall. That's because if the height is infinitesimally different from 8, like 8.000000...0001 m, it would not count as a favorauble outcome. So if we calculate it in the continuous case, we would get $P(8)\\approx 0$. What is more useful is to ask, instead, what is the probability of an outcome being between a certain range $x$ and $x+\\Delta x$. \n",
    "If the $\\Delta x$ is very small, we can write:\n",
    "\n",
    "$$\n",
    "\\Delta x \\to dx, \\qquad P(x\\le X \\le x+dx) \\to p(x)dx\n",
    "$$\n",
    "\n",
    "where $p(x)$ (notice the lowercase) is a continuous function describing the shape of the distribution, called **probability density**.\n",
    "\n",
    "Then to get the total probability in a certain range one can sum all the infinitesimal probabilities:\n",
    "\n",
    "$$\n",
    "P(x\\le X \\le x+\\Delta x)=\\int_x^{x+\\Delta x} p(x)dx\n",
    "$$\n",
    "\n",
    "Similarly to the discrete distributions, the normalization condition is:\n",
    "$$\n",
    "\\int_{-\\infty}^{+\\infty} p(x)dx=1\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average, variance and std in continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now that we have a framework for continuous distributions, we can say that the central limit theorem describes the probability density for a Gaussian stochastic process:\n",
    "\n",
    "$$\n",
    "p(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-<x>)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "where $<x>$ is the mean and $\\sigma$ the standard deviation.\n",
    "\n",
    "\n",
    "\n",
    "Estimates for $\\mu$ and $\\sigma$ from discrete measurements: mean, corrected sampled standard deviation, root mean squared error\n",
    "Standard deviation of the mean\n",
    "\n",
    "### Back to errors: which one to choose?\n",
    "Say the sensitivity of an instrument is $s$. By using this instrument we collect many data points and we calculate the standard deviation $\\sigma$. Which value to use as an uncertainty?\n",
    "\n",
    "The short answer is: the largest value between $s$ and $\\sigma$. That's because if the standard deviation is below the sensitivity, the individual variations may just be due to the imprecision of the instrument and not to a natural random variability. However, if the standard deviation is greater than the sensitivity, we know that the individual variations are all well measured with the instrument, despite its limited resolution.\n",
    "\n",
    "# Error propagation\n",
    "Recap: Taylor expansion\n",
    "Taylor expansion of the velocity\n",
    "General formula\n",
    "Special case of power laws"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
