{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty and errors\n",
    "If there needs to be one thing you must remember from this lesson, let it be this: every measurement or prediction in physics comes with an associated uncertainty. Always, no exception. That is because measurements provide an *estimate* of the true value, not the true value itself. \n",
    "\n",
    "In the context of quantitative measurements, the words **uncertainty** and **error** are sometimes used interchangeably. To be precise, error generally refers to the difference between a measured value and the true or accepted value, while uncertainty is an estimate of the range within which the true value is likely to lie, considering all known errors.\n",
    "\n",
    "We cannot stress it enough: uncertainty is always present, whether we are reporting a measurement or making a prediction from the theory. Therefore it is important to know how to handle it quantitatively, regardless of where your interest in biophysics will take you.\n",
    "\n",
    "> **Philosophical digression**\n",
    ">\n",
    ">But what about the fundamental constants on Nature, like for example the speed of light in vacuum? Do they come with an uncertainty too? In principle no, but only because they become [axioms](https://en.wikipedia.org/wiki/Axiom) of the physical framework used. For this we must be certain that they are *really* constant. In other words, whenever measured properly, the obtained value must always be within the range of uncertainty. In that case and that one only, it can be considered a fixed value of Nature and reported without error. The idea that, if we use better, more sensitive measuring tools, we would still get the same value, just with a smaller uncertainty.\n",
    ">\n",
    ">Not for nothing, the speed of light in vacuum is so constant it is used together with the definition of [second](https://en.wikipedia.org/wiki/Second) to define indirectly what the [meter](https://en.wikipedia.org/wiki/Metre) is. In the next chapter we will talk about units and dimensions, and we will come back at it.\n",
    ">\n",
    "\n",
    "### Types of errors\n",
    "There are countless reasons why we cannot be certain about the values we measure. Consider for instance the measuring of the length of the table: maybe the length changes slightly from side to side due to imperfect manufacturing, or the meter we use cannot reach micrometric precision, or we accidentally measure between two points that are not exactly parallel to the length, or some evil crazy scientist replaced the meter with an inaccurate one, and so on. Some of this errors can be prevented, some other cannot. This does not mean that we cannot be precise in our scientific claims, but only that we acknowledge that what we know is limited to the tools we have to explore Nature. \n",
    "\n",
    "Errors are generally classified between these categories:\n",
    "- **Random errors**: Variations that occur unpredictably from one measurement to another, mostly due to the nature of the process. They cannot be minimized at the source, but they can be reduced by averaging over a large number of observations. An example are the differences in protein concentrations expressed by the same cell line.\n",
    "- **Systematic errors**: These are consistent, repeatable errors due to flaws in measurement instruments or procedures. Most systematic errors take the form of *biases*. They can and should be minimized at the source by properly calibrating the system. Unlike random errors, systematic errors cannot be reduced by increasing the number of observations. An example is not accounting for the mass of the weighing paper when measuring the weight of a powder.\n",
    "- **Blunders**: These are careless human errors that happen randomly and result in (completely) off results. Compared to systematic errors, blunders are due to tiredness or negligence and generate biases hard to quantify. They can be minimized with good attention, careful practices and supervision of other colleagues who can point out the obvious mistakes. An example is pipetting $1\\,mL$ of trypsin instead of $1\\,\\mu L$, or recording a $9$ instead of a $6$ onto a lab report.\n",
    "\n",
    "Some textbooks do not distinguish between systematic errors and blunders; here I want to stress that, while systematic errors can in some cases be corrected in retrospective, blunders should be categorically excluded from analysis.\n",
    " \n",
    "### Instrument resolution\n",
    "Take a ruler and ask yourself: can I measure micrometers with this?\n",
    "\n",
    "The answer is no. That is because the spacing between ticks on the ruler is $1\\,mm$: finer ticks would be hard if not impossible to see with the naked eye, and due to [parallax](https://en.wikipedia.org/wiki/Parallax) they would literally be useless as different observers would report different measurements.\n",
    "\n",
    "The instrument resolution (sometimes called *sensitivity*) represents the smallest increment an instrument can reliably measure, its \"least count\".\n",
    "\n",
    "If we are measuring the length of a table, even after removing all other sources of errors, we cannot get an estimate of the true value more precise than the resolution of the measuring tool used. The only way to reduce this uncertainty is to use more sensitive tools with better resolution.\n",
    "\n",
    "### Correctly reporting a measurement\n",
    "The usual way to report a physical quantity $X$ with its best estimate $x$ and its error $e_x$ is:\n",
    "\n",
    "$$\n",
    "X = x \\pm e_x\n",
    "$$\n",
    "\n",
    "Imagine you have measured a biophysical property and you estimated its value to be $x=3.29784625$. You also estimate the uncertainty on this measure to be $e_x=0.03527$. How do we report the result correctly?\n",
    "\n",
    "First of all, we have to look at the error. How do we know its value is not $0.03526$ or $0.03528$? How sure are we on that last digit? Logically we would need to introduce the *error on the error*, but then how well do we know it? Continue like this and you'll end up defining infinite errors iteratively. Now, this approach is not useful to anyone. For this reason we need to slightly approximate the error, and round it to its most significant digits only. The terms significant digits and significant figures can be used, in this context, equivalently.\n",
    "\n",
    "There are precise **rounding rules** that are worth memorizing. The first zeros are important because they tell us the order of magnitude of the error. If there are no zeros on the left side of the number, then we simply don't need to consider them. Then we look at the first non-zero digit and the one after it:\n",
    "- If the first digit is a 1 and the second is not 0, we keep both digits and discard all the other afterwards;\n",
    "- If the first digit is a 1 and the second is 0, we keep only the first digit;\n",
    "- If the first digit is not a 1 and the second is between 0 and 4 (included), then we **truncate** everything after the first digit;\n",
    "- If the first digit is not a 1 and the second is between 5 and 9 (included), then we **round up**, meaning we add 1 to the first digit and truncate all the following digits.\n",
    "\n",
    "```{figure} ../figures/chap1_rounding.png\n",
    "---\n",
    "width: 80%\n",
    "name: 1_rounding\n",
    "align: center\n",
    "---\n",
    "```\n",
    "\n",
    "So, in our example, the first non-zero digit is 3, followed by a 5. We round up the 5, so the 3 becomes a 4, and we will finally have $e_x=0.04$.\n",
    "\n",
    "Now we look at the value $x$. If we cannot know the error better than a certain amount of significant digits, we certainly cannot know the value to that precision either. So we count how many digits <ins>the error</ins> has after the dot, and round up or down the value $x$ to have the same amount of digits.\n",
    "\n",
    "The rounding rules there still hold:\n",
    "- If the first digit after the last significant digit is between 0 and 4 (included), then we **truncate** everything after the first digit;\n",
    "- If the first digit after the last significant digit is between 5 and 9 (included), then we **round up**, meaning we add 1 to the first digit and truncate all the following digits.\n",
    "\n",
    "In the example, $x=\\mathbf{3.29}784625$ is marked in bold to have the same amount of digits as the error. Since the first non-significant digit is a 7, we add one to the last significant digit. But because 9+1 becomes 10, the value to report will be $x=3.30$. The last zero is still significant, so we need to report it.\n",
    "\n",
    "All in all, the correct way to report this quantity is:\n",
    "\n",
    "$$\n",
    " X = 3.30\\pm 0.04\n",
    "$$\n",
    "\n",
    "Much better, isn't it?\n",
    "\n",
    "If at this point you are still a bit confused, I suggest you to practice with some exercises, because correctly reporting measurements and predictions is one of the most fundamental skills in physics.\n",
    "\n",
    "### Statistics of random errors\n",
    "\n",
    "> **Note**\n",
    ">\n",
    ">In the following section you will find some interactive tools to help you visualize the concepts presented.\n",
    ">To run the simulations you first need to click {fa}`rocket` --> {guilabel}`Live Code` on the top right corner of this page, and wait until the kernel has loaded. Then, you need to run the block of code by clicking  {guilabel}`run`.\n",
    ">\n",
    "\n",
    "When we say the outcome of a measurement is \"random\", we do not imply \"unquantifiable\". It is possible that the same object gives different outcomes when measured sequentially in time, or that within a group of objects the measured properties vary in a predetermined range, or both. What is the true value in these cases? What is the uncertainty? \n",
    "\n",
    "Consider for example the height distribution of various tree species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bokeh'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbokeh\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplotting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m figure, show, output_notebook\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbokeh\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnDataSource, CustomJS\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbokeh\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MouseMove\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'bokeh'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, CustomJS, Span, BoxAnnotation, Label\n",
    "from bokeh.events import MouseMove\n",
    "\n",
    "# For inline display in a notebook; if running as a script, consider using output_file(\"interactive.html\")\n",
    "output_notebook()\n",
    "\n",
    "# Load the CSV file and clean column names\n",
    "df = pd.read_csv(\"tree_datasets.csv\")\n",
    "df.columns = [col.strip() for col in df.columns if col.strip() != \"\"]\n",
    "\n",
    "# Define coordinate pairs based on header names: (x, y)\n",
    "pairs = [\n",
    "    (\"Ix\", \"Illicium verum\"),\n",
    "    (\"Mx\", \"Masson pine\"),\n",
    "    (\"Cx\", \"Chinese fir\"),\n",
    "    (\"Sx\", \"Splash pine\")\n",
    "]\n",
    "\n",
    "# Define the Okabe–Ito color palette (for 4 curves)\n",
    "okabe_colors = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\"]\n",
    "\n",
    "# Prepare lists for the renderers and annotation objects, and to store each curve's weighted mean and label\n",
    "renderers = []\n",
    "spans = []\n",
    "left_bands = []\n",
    "right_bands = []\n",
    "labels = []  # for showing mean and std info\n",
    "means = []   # weighted means for each curve\n",
    "\n",
    "# Create a Bokeh figure and set axis labels\n",
    "p = figure(title=\"Tree Height Distribution in Southern China\", width=800, height=400, tools=\"\")\n",
    "p.xaxis.axis_label = \"Height [m]\"\n",
    "p.yaxis.axis_label = \"Relative frequency [a.u.]\"\n",
    "\n",
    "for i, (xcol, ycol) in enumerate(pairs):\n",
    "    if xcol in df.columns and ycol in df.columns:\n",
    "        color = okabe_colors[i]\n",
    "        # Create a data source for the curve\n",
    "        source = ColumnDataSource(data=dict(x=df[xcol], y=df[ycol]))\n",
    "        # Plot the curve with the chosen color\n",
    "        r = p.line('x', 'y', source=source, line_width=2, alpha=1.0, line_color=color, legend_label=f\"{ycol}\")\n",
    "        renderers.append(r)\n",
    "        \n",
    "        # Compute the weighted mean and standard deviation (using y as frequency)\n",
    "        weights = df[ycol]\n",
    "        xs = df[xcol]\n",
    "        mean_val = (xs * weights).sum() / weights.sum()\n",
    "        means.append(mean_val)  # Save for later use in the callback\n",
    "        std_val = np.sqrt((weights * (xs - mean_val)**2).sum() / weights.sum())\n",
    "        \n",
    "        # Interpolate to find the y-value at x = mean_val\n",
    "        mean_y = np.interp(mean_val, xs, df[ycol])\n",
    "        \n",
    "        # Create a vertical dashed line at the weighted mean\n",
    "        span = Span(location=mean_val, dimension='height', line_color=color,\n",
    "                    line_dash='dashed', line_width=2, visible=False)\n",
    "        # Create BoxAnnotations for ±1 standard deviation around the mean\n",
    "        left_band = BoxAnnotation(left=mean_val - std_val, right=mean_val,\n",
    "                                  fill_color=color, fill_alpha=0.2, visible=False)\n",
    "        right_band = BoxAnnotation(left=mean_val, right=mean_val + std_val,\n",
    "                                   fill_color=color, fill_alpha=0.2, visible=False)\n",
    "        # Create a Label displaying the mean (as <h>) and std (as σ)\n",
    "        label_text = \"<h>: {:.2f}, σ: {:.2f}\".format(mean_val, std_val)\n",
    "        label = Label(x=mean_val, y=mean_y, x_offset=30, y_offset=5,\n",
    "                      text=label_text, text_color=color, text_font_size=\"10pt\", visible=False)\n",
    "        \n",
    "        # Add annotations and label to the plot\n",
    "        p.add_layout(span)\n",
    "        p.add_layout(left_band)\n",
    "        p.add_layout(right_band)\n",
    "        p.add_layout(label)\n",
    "        \n",
    "        spans.append(span)\n",
    "        left_bands.append(left_band)\n",
    "        right_bands.append(right_band)\n",
    "        labels.append(label)\n",
    "    else:\n",
    "        print(f\"Column pair ({xcol}, {ycol}) not found in DataFrame.\")\n",
    "\n",
    "# CustomJS callback: highlight the curve whose weighted mean is closest to the mouse's x position.\n",
    "callback_code = \"\"\"\n",
    "let best_index = -1;\n",
    "let best_dist = Infinity;\n",
    "for (let i = 0; i < means.length; i++) {\n",
    "    const dist = Math.abs(cb_obj.x - means[i]);\n",
    "    if (dist < best_dist) {\n",
    "        best_dist = dist;\n",
    "        best_index = i;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Update each curve's opacity and the visibility of its annotations based on the best match.\n",
    "for (let i = 0; i < renderers.length; i++) {\n",
    "    if (i === best_index) {\n",
    "        renderers[i].glyph.line_alpha = 1.0;\n",
    "        spans[i].visible = true;\n",
    "        left_bands[i].visible = true;\n",
    "        right_bands[i].visible = true;\n",
    "        labels[i].visible = true;\n",
    "    } else {\n",
    "        renderers[i].glyph.line_alpha = 0.25;\n",
    "        spans[i].visible = false;\n",
    "        left_bands[i].visible = false;\n",
    "        right_bands[i].visible = false;\n",
    "        labels[i].visible = false;\n",
    "    }\n",
    "}\n",
    "p.change.emit();\n",
    "\"\"\"\n",
    "\n",
    "# Create and attach the callback; pass the renderers, annotations, labels, and means to the JS callback.\n",
    "callback = CustomJS(args=dict(renderers=renderers, spans=spans, left_bands=left_bands, \n",
    "                              right_bands=right_bands, labels=labels, p=p, means=means), code=callback_code)\n",
    "p.js_on_event(MouseMove, callback)\n",
    "\n",
    "p.legend.location = \"top_left\"\n",
    "p.legend.click_policy = \"hide\"\n",
    "\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Data from Wu, Y.; Zhang, X. Object-Based Tree Species Classification Using Airborne Hyperspectral Images and LiDAR Data. Forests 2020, 11, 32. https://doi.org/10.3390/f11010032)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still describe some general properties of each species, the most obvious being the average height and the spread of the distribution.\n",
    "\n",
    "Probability to measure a certain value\n",
    "Probability density $p(x)$\n",
    "Mean, variance, standard deviation\n",
    "Model for $p(x)$: Gaussian distribution with parameters $\\mu$ and $\\sigma$\n",
    "Estimates for $\\mu$ and $\\sigma$ from discrete measurements: mean, corrected sampled standard deviation, root mean squared error\n",
    "Standard deviation of the mean: Central limit theorem\n",
    "Standard error of the mean$m$\n",
    "Relative error\n",
    "\n",
    "### Which error to choose?\n",
    "Say the sensitivity of an instrument is $s$. By using this instrument we collect many data points and we calculate the standard deviation $\\sigma$. Which value to use as an uncertainty?\n",
    "\n",
    "The short answer is: the largest value between $s$ and $\\sigma$. That's because if the standard deviation is below the sensitivity, the individual variations may just be due to the imprecision of the instrument and not to a natural random variability. However, if the standard deviation is greater than the sensitivity, we know that the individual variations are all well measured with the instrument, despite its limited resolution.\n",
    "\n",
    "### Those darn outliers\n",
    "a\n",
    "\n",
    "# Error propagation\n",
    "Recap: Taylor expansion\n",
    "Taylor expansion of the velocity\n",
    "General formula\n",
    "Special case of power laws"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
